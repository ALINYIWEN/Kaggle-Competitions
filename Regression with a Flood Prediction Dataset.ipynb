{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression with a Flood Prediction Dataset 洪水預測\n",
    "- 目標是根據各種因素預測某個地區發生洪水的機率。\n",
    "- 洪水檢測是指識別、監控特定區域是否有洪水或可能發生洪水並向當局或個人發出警報的過程。它涉及使用各種技術和方法來檢測、預測和減輕洪水的影響。\n",
    "\n",
    "### 評估\n",
    "- 洪水發生的機率值 (FloodProbability) \n",
    "- 使用 R2 分數對提交的內容進行評估。\n",
    "\n",
    "### 特徵定義\n",
    "- MonsoonIntensity: 季風強度\n",
    "- TopographyDrainage: 地形排水\n",
    "- RiverManagement: 河流管理\n",
    "- Deforestation: 森林破壞\n",
    "- Urbanization: 都市化程度\n",
    "- ClimateChange: 氣候變化\n",
    "- DamsQuality: 水壩品質\n",
    "- Siltation: 泥沙淤積\n",
    "- AgriculturalPractices: 農業實踐\n",
    "- Encroachments: 土地侵占\n",
    "- IneffectiveDisasterPreparedness: 災害準備不足\n",
    "- DrainageSystems: 排水系統\n",
    "- CoastalVulnerability: 海岸脆弱性\n",
    "- Landslides: 山崩情況\n",
    "- Watersheds: 水域集水區\n",
    "- DeterioratingInfrastructure: 基礎設施惡化\n",
    "- PopulationScore: 人口密度指數\n",
    "- WetlandLoss: 濕地損失\n",
    "- InadequatePlanning: 計畫不足\n",
    "- PoliticalFactors: 政治因素\n",
    "- FloodProbability: 洪水發生機率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from pandas_summary import DataFrameSummary\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler , StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, make_scorer\n",
    "from scipy.stats import randint, uniform # 离散的隨機整數, 連續均勻分布的隨機變數\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "plt.rc('font', family = 'Microsoft JhengHei')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 載入資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data/Regression with a Flood Prediction Dataset/train.csv')\n",
    "df_test = pd.read_csv('./data/Regression with a Flood Prediction Dataset/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建構新特徵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>季風強度</th>\n",
       "      <th>地形排水</th>\n",
       "      <th>河川管理</th>\n",
       "      <th>森林砍伐</th>\n",
       "      <th>都市化</th>\n",
       "      <th>氣候變遷</th>\n",
       "      <th>大壩質量</th>\n",
       "      <th>淤積</th>\n",
       "      <th>農業實踐</th>\n",
       "      <th>侵害</th>\n",
       "      <th>...</th>\n",
       "      <th>排水系統</th>\n",
       "      <th>海岸脆弱性</th>\n",
       "      <th>山崩</th>\n",
       "      <th>分水嶺</th>\n",
       "      <th>基礎設施惡化</th>\n",
       "      <th>人口分數</th>\n",
       "      <th>濕地損失</th>\n",
       "      <th>規劃不充分</th>\n",
       "      <th>政治因素</th>\n",
       "      <th>洪水機率</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   季風強度  地形排水  河川管理  森林砍伐  都市化  氣候變遷  大壩質量  淤積  農業實踐  侵害  ...  排水系統  海岸脆弱性  \\\n",
       "0     5     8     5     8    6     4     4   3     3   4  ...     5      3   \n",
       "1     6     7     4     4    8     8     3   5     4   6  ...     7      2   \n",
       "2     6     5     6     7    3     7     1   5     4   5  ...     7      3   \n",
       "3     3     4     6     5    4     8     4   7     6   8  ...     2      4   \n",
       "4     5     3     2     6    4     4     3   3     3   3  ...     2      2   \n",
       "\n",
       "   山崩  分水嶺  基礎設施惡化  人口分數  濕地損失  規劃不充分  政治因素   洪水機率  \n",
       "0   3    5       4     7     5      7     3  0.445  \n",
       "1   0    3       5     3     3      4     3  0.450  \n",
       "2   7    5       6     8     2      3     3  0.530  \n",
       "3   7    4       4     6     5      7     5  0.535  \n",
       "4   6    6       4     1     2      3     5  0.415  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.drop(\"id\", axis = 1, inplace = True)\n",
    "df_test.drop(\"id\", axis = 1, inplace = True)\n",
    "\n",
    "# 新列名的字典對應\n",
    "new_columns = {\n",
    "    'MonsoonIntensity': '季風強度',\n",
    "    'TopographyDrainage': '地形排水',\n",
    "    'RiverManagement': '河川管理',\n",
    "    'Deforestation': '森林砍伐',\n",
    "    'Urbanization': '都市化',\n",
    "    'ClimateChange': '氣候變遷',\n",
    "    'DamsQuality': '大壩質量',\n",
    "    'Siltation': '淤積',\n",
    "    'AgriculturalPractices': '農業實踐',\n",
    "    'Encroachments': '侵害',\n",
    "    'IneffectiveDisasterPreparedness': '災難準備不力',\n",
    "    'DrainageSystems': '排水系統',\n",
    "    'CoastalVulnerability': '海岸脆弱性',\n",
    "    'Landslides': '山崩',\n",
    "    'Watersheds': '分水嶺',\n",
    "    'DeterioratingInfrastructure': '基礎設施惡化',\n",
    "    'PopulationScore': '人口分數',\n",
    "    'WetlandLoss': '濕地損失',\n",
    "    'InadequatePlanning': '規劃不充分',\n",
    "    'PoliticalFactors': '政治因素',\n",
    "    'FloodProbability': '洪水機率'\n",
    "}\n",
    "\n",
    "# 使用 rename 方法進行批量更改\n",
    "df_train = df_train.rename(columns = new_columns)\n",
    "df_test = df_test.rename(columns = new_columns)\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>季風強度</th>\n",
       "      <th>地形排水</th>\n",
       "      <th>河川管理</th>\n",
       "      <th>森林砍伐</th>\n",
       "      <th>都市化</th>\n",
       "      <th>氣候變遷</th>\n",
       "      <th>大壩質量</th>\n",
       "      <th>淤積</th>\n",
       "      <th>農業實踐</th>\n",
       "      <th>侵害</th>\n",
       "      <th>...</th>\n",
       "      <th>排水系統</th>\n",
       "      <th>海岸脆弱性</th>\n",
       "      <th>山崩</th>\n",
       "      <th>分水嶺</th>\n",
       "      <th>基礎設施惡化</th>\n",
       "      <th>人口分數</th>\n",
       "      <th>濕地損失</th>\n",
       "      <th>規劃不充分</th>\n",
       "      <th>政治因素</th>\n",
       "      <th>洪水機率</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "      <td>1117957.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.92145</td>\n",
       "      <td>4.926671</td>\n",
       "      <td>4.955322</td>\n",
       "      <td>4.94224</td>\n",
       "      <td>4.942517</td>\n",
       "      <td>4.934093</td>\n",
       "      <td>4.955878</td>\n",
       "      <td>4.927791</td>\n",
       "      <td>4.942619</td>\n",
       "      <td>4.94923</td>\n",
       "      <td>...</td>\n",
       "      <td>4.946893</td>\n",
       "      <td>4.953999</td>\n",
       "      <td>4.931376</td>\n",
       "      <td>4.929032</td>\n",
       "      <td>4.925907</td>\n",
       "      <td>4.92752</td>\n",
       "      <td>4.950859</td>\n",
       "      <td>4.940587</td>\n",
       "      <td>4.939004</td>\n",
       "      <td>0.50448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.056387</td>\n",
       "      <td>2.093879</td>\n",
       "      <td>2.072186</td>\n",
       "      <td>2.051689</td>\n",
       "      <td>2.083391</td>\n",
       "      <td>2.057742</td>\n",
       "      <td>2.083063</td>\n",
       "      <td>2.065992</td>\n",
       "      <td>2.068545</td>\n",
       "      <td>2.083324</td>\n",
       "      <td>...</td>\n",
       "      <td>2.072333</td>\n",
       "      <td>2.088899</td>\n",
       "      <td>2.078287</td>\n",
       "      <td>2.082395</td>\n",
       "      <td>2.064813</td>\n",
       "      <td>2.074176</td>\n",
       "      <td>2.068696</td>\n",
       "      <td>2.081123</td>\n",
       "      <td>2.09035</td>\n",
       "      <td>0.051026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>...</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "      <td>1117957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>...</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "      <td>numeric</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   季風強度       地形排水       河川管理       森林砍伐        都市化  \\\n",
       "count         1117957.0  1117957.0  1117957.0  1117957.0  1117957.0   \n",
       "mean            4.92145   4.926671   4.955322    4.94224   4.942517   \n",
       "std            2.056387   2.093879   2.072186   2.051689   2.083391   \n",
       "min                 0.0        0.0        0.0        0.0        0.0   \n",
       "25%                 3.0        3.0        4.0        4.0        3.0   \n",
       "50%                 5.0        5.0        5.0        5.0        5.0   \n",
       "75%                 6.0        6.0        6.0        6.0        6.0   \n",
       "max                16.0       18.0       16.0       17.0       17.0   \n",
       "counts          1117957    1117957    1117957    1117957    1117957   \n",
       "uniques              17         19         17         18         18   \n",
       "missing               0          0          0          0          0   \n",
       "missing_perc         0%         0%         0%         0%         0%   \n",
       "types           numeric    numeric    numeric    numeric    numeric   \n",
       "\n",
       "                   氣候變遷       大壩質量         淤積       農業實踐         侵害  ...  \\\n",
       "count         1117957.0  1117957.0  1117957.0  1117957.0  1117957.0  ...   \n",
       "mean           4.934093   4.955878   4.927791   4.942619    4.94923  ...   \n",
       "std            2.057742   2.083063   2.065992   2.068545   2.083324  ...   \n",
       "min                 0.0        0.0        0.0        0.0        0.0  ...   \n",
       "25%                 3.0        4.0        3.0        3.0        4.0  ...   \n",
       "50%                 5.0        5.0        5.0        5.0        5.0  ...   \n",
       "75%                 6.0        6.0        6.0        6.0        6.0  ...   \n",
       "max                17.0       16.0       16.0       16.0       18.0  ...   \n",
       "counts          1117957    1117957    1117957    1117957    1117957  ...   \n",
       "uniques              18         17         17         17         19  ...   \n",
       "missing               0          0          0          0          0  ...   \n",
       "missing_perc         0%         0%         0%         0%         0%  ...   \n",
       "types           numeric    numeric    numeric    numeric    numeric  ...   \n",
       "\n",
       "                   排水系統      海岸脆弱性         山崩        分水嶺     基礎設施惡化  \\\n",
       "count         1117957.0  1117957.0  1117957.0  1117957.0  1117957.0   \n",
       "mean           4.946893   4.953999   4.931376   4.929032   4.925907   \n",
       "std            2.072333   2.088899   2.078287   2.082395   2.064813   \n",
       "min                 0.0        0.0        0.0        0.0        0.0   \n",
       "25%                 4.0        3.0        3.0        3.0        3.0   \n",
       "50%                 5.0        5.0        5.0        5.0        5.0   \n",
       "75%                 6.0        6.0        6.0        6.0        6.0   \n",
       "max                17.0       17.0       16.0       16.0       17.0   \n",
       "counts          1117957    1117957    1117957    1117957    1117957   \n",
       "uniques              18         18         17         17         18   \n",
       "missing               0          0          0          0          0   \n",
       "missing_perc         0%         0%         0%         0%         0%   \n",
       "types           numeric    numeric    numeric    numeric    numeric   \n",
       "\n",
       "                   人口分數       濕地損失      規劃不充分       政治因素       洪水機率  \n",
       "count         1117957.0  1117957.0  1117957.0  1117957.0  1117957.0  \n",
       "mean            4.92752   4.950859   4.940587   4.939004    0.50448  \n",
       "std            2.074176   2.068696   2.081123    2.09035   0.051026  \n",
       "min                 0.0        0.0        0.0        0.0      0.285  \n",
       "25%                 3.0        4.0        3.0        3.0       0.47  \n",
       "50%                 5.0        5.0        5.0        5.0      0.505  \n",
       "75%                 6.0        6.0        6.0        6.0       0.54  \n",
       "max                18.0       19.0       16.0       16.0      0.725  \n",
       "counts          1117957    1117957    1117957    1117957    1117957  \n",
       "uniques              19         20         17         17         83  \n",
       "missing               0          0          0          0          0  \n",
       "missing_perc         0%         0%         0%         0%         0%  \n",
       "types           numeric    numeric    numeric    numeric    numeric  \n",
       "\n",
       "[13 rows x 21 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 創建摘要\n",
    "feature_numerical = [feature for feature in df_train.columns if df_train[feature].dtypes != 'O']\n",
    "df_summary = DataFrameSummary(df_train[feature_numerical])\n",
    "\n",
    "# 顯示摘要統計信息\n",
    "df_summary.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 異常值檢查\n",
    "- 所有特徵均有超出最大值的異常值點, 判斷這些條件均有可能發生, 因此不移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature_numerical 取得數值型特徵\n",
    "# feature_numerical = [feature for feature in df_train.columns if df_train[feature].dtypes != 'O']\n",
    "\n",
    "# # Boxplot\n",
    "# fig, axes = plt.subplots(7,3)\n",
    "# sns.set_theme(rc = {'figure.figsize' : (20, 15)})\n",
    "# row, col  = 0, 0\n",
    "# while row < 7:\n",
    "#     for feature in feature_numerical:\n",
    "#         sns.boxplot(x = feature, data = df_train, ax = axes[row, col], color = '#047794')\n",
    "#         col = col + 1\n",
    "#         if col == 3:\n",
    "#             row = row + 1\n",
    "#             col = 0\n",
    "# plt.tight_layout(h_pad = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA - 數值特徵分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 觀察預測目標"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='洪水機率', ylabel='Count'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rc('font', family = 'Microsoft JhengHei')\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.histplot(df_train['洪水機率'], color = '#047794', kde = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 明顯所有特徵均是程度越高, 發生洪水的機率就越高\n",
    "- 簡單來說, 地形破壞越嚴重, 災害就越嚴重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 觀察數值類型的特徵\n",
    "# plt.rc('font', family = 'Microsoft JhengHei')\n",
    "# plt.figure(figsize=(20,20))\n",
    "\n",
    "# for col, source in enumerate(df_train):\n",
    "#     if source == \"洪水機率\":\n",
    "#         break\n",
    "#     plt.subplot(5, 4, col+ 1)\n",
    "    \n",
    "#     # scatterplot\n",
    "#     sns.scatterplot( x = source, y = \"洪水機率\", data = df_train, color = '#047794')\n",
    "\n",
    "#     plt.tight_layout(h_pad = 3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape : (1117957, 20)\n",
      "y_train.shape : (1117957,)\n",
      "test.shape : (745305, 20)\n"
     ]
    }
   ],
   "source": [
    "df_original = pd.read_csv('./data/Regression with a Flood Prediction Dataset/flood.csv')\n",
    "train = pd.concat([df_train, df_original]).drop_duplicates()\n",
    "test = df_test\n",
    "\n",
    "# 使用StandardScaler對訓練數據進行標準化轉換\n",
    "scaler = StandardScaler()\n",
    "train_scaled = scaler.fit_transform(df_train.drop(columns = ['洪水機率']))\n",
    "test_scaled = scaler.transform(df_test)\n",
    "\n",
    "# 將標準化後的訓練數據轉換回DataFrame，以保留特徵名稱\n",
    "train_scaled_df = pd.DataFrame(train_scaled, columns=df_train.drop(columns=['洪水機率']).columns)\n",
    "test_scaled_df = pd.DataFrame(test_scaled, columns=df_test.columns)\n",
    "\n",
    "X_train = train_scaled_df\n",
    "y_train = df_train['洪水機率']\n",
    "test = test_scaled_df\n",
    "\n",
    "print(f'X_train.shape : {X_train.shape}')\n",
    "print(f'y_train.shape : {y_train.shape}')\n",
    "print(f'test.shape : {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.015471 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.053295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.018529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071478 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074942 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067044 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073594 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076931 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070082 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.113380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064868 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066650 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084149 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075158 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069146 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067857 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.057665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067968 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066736 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062783 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067970 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063603 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059505 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.059787 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.055747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067312 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 370\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504471\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072072 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 894365, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504463\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 368\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504470\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.114668 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504528\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193909 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 366\n",
      "[LightGBM] [Info] Number of data points in the train set: 894366, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504469\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.143071 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 367\n",
      "[LightGBM] [Info] Number of data points in the train set: 1117957, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 0.504480\n",
      "Best parameters found:  {'learning_rate': 0.04460492588220279, 'max_depth': 11, 'n_estimators': 729}\n",
      "Best R2 score:  0.8402041788673852\n",
      "Best MSE score:  0.0004160416284701692\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "LightGBM R2 score: 0.8462310633508118\n",
      "LightGBM MSE: 0.0004003620411270992\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 評分函數\n",
    "scoring = {'r2': make_scorer(r2_score), 'mse': make_scorer(mean_squared_error)}\n",
    "\n",
    "# 超參數\n",
    "param_dist = {\n",
    "    'max_depth': randint(5, 20),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'n_estimators': randint(100, 1000)\n",
    "}\n",
    "\n",
    "lgb = LGBMRegressor()\n",
    "random_search = RandomizedSearchCV(estimator = lgb, param_distributions = param_dist, scoring = scoring, refit = 'r2', n_iter = 10, cv = 20)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "y_train_predict = random_search.predict(X_train)\n",
    "\n",
    "# 输出预测指标\n",
    "print('LightGBM R2 score:', r2_score(y_train, y_train_predict))\n",
    "print('LightGBM MSE:', mean_squared_error(y_train, y_train_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 預測並輸出結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "y_test_predict = random_search.predict(test)\n",
    "\n",
    "sub = pd.read_csv('./data/Regression with a Flood Prediction Dataset/sample_submission.csv')\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = sub['id']\n",
    "submission['FloodProbability'] = y_test_predict\n",
    "submission.to_csv('./data/Regression with a Flood Prediction Dataset/submission_lgbm.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
